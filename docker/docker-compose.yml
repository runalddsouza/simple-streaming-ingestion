x-superset-image: &superset-image apache/superset:4.0.2
x-superset-depends-on: &superset-depends-on
  - superset-db
  - redis
  - hiveserver2
x-superset-volumes:
  &superset-volumes
  - ./superset:/app/docker
  - ./superset/superset_config.py:/app/superset/config.py
  - superset_home:/app/superset_home

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.7.1
    hostname: zookeeper
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - cluster

  broker:
    image: confluentinc/cp-server:7.7.1
    hostname: broker
    container_name: broker
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1
      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: broker:29092
      CONFLUENT_METRICS_REPORTER_ZOOKEEPER_CONNECT: zookeeper:2181
      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1
      CONFLUENT_METRICS_ENABLE: "true"
      CONFLUENT_SUPPORT_CUSTOMER_ID: "anonymous"
    command: >
      /bin/sh -c "((sleep 15 && kafka-topics --create --zookeeper zookeeper:2181 --replication-factor 1 --partitions 3 --topic FootballMatchEvent)&) &&
      /etc/confluent/docker/run"
    networks:
      - cluster

  event_producer-1:
    image: python:3.11-slim-buster
    container_name: event_producer_1
    depends_on:
      - broker
    volumes:
      - ../:/usr/local/app
    environment:
      - PYTHONPATH=$PYTHONPATH:/usr/local/app/
    command: >
      bash -c "pip install -r /usr/local/app/football_match_event_producer/requirements.txt &&
      python /usr/local/app/football_match_event_producer/src/app.py --topic FootballMatchEvent --home 'FC Bayern MÃ¼nchen' --away 'Real Madrid CF' --bootstrap-servers broker:29092 --log-file /usr/local/app.log"
    networks:
      - cluster

  event_producer-2:
    image: python:3.11-slim-buster
    container_name: event_producer_2
    depends_on:
      - broker
    volumes:
      - ../:/usr/local/app
    environment:
      - PYTHONPATH=$PYTHONPATH:/usr/local/app/
    command: >
      bash -c "pip install -r /usr/local/app/football_match_event_producer/requirements.txt &&
      python /usr/local/app/football_match_event_producer/src/app.py --topic FootballMatchEvent --home 'Milan' --away 'Liverpool' --bootstrap-servers broker:29092 --log-file /usr/local/app.log"
    networks:
      - cluster

  namenode:
    image: apache/hadoop:3.4.0
    container_name: namenode
    hostname: namenode
    restart: on-failure
    user: root
    ports:
      - "9870:9870"
      - "8080:8080"
    command: [ "/bin/bash", "/namenode-yarn.sh" ]
    environment:
      - HADOOP_HOME=/opt/hadoop
      - HADOOP_CLASSPATH=/opt/hadoop/jars/
    volumes:
      - ./hadoop/namenode-yarn.sh:/namenode-yarn.sh
      - ./hadoop/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
      - ./hadoop/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
      - ./hadoop/yarn-site.xml:/opt/hadoop/etc/hadoop/yarn-site.xml
      - ./hadoop/mapred-site.xml:/opt/hadoop/etc/hadoop/mapred-site.xml
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://namenode:9870" ]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - cluster

  datanode:
    image: apache/hadoop:3.4.0
    container_name: datanode
    hostname: datanode
    restart: on-failure
    user: root
    ports:
      - "9864:9864"
      - "8042:8042"
    command: [ "/bin/bash", "/datanode.sh" ]
    environment:
      - HADOOP_HOME=/opt/hadoop
      - HADOOP_CLASSPATH=/opt/hadoop/jars/
    volumes:
      - ./hadoop/datanode.sh:/datanode.sh
      - ./hadoop/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
      - ./hadoop/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
      - ./hadoop/yarn-site.xml:/opt/hadoop/etc/hadoop/yarn-site.xml
      - ./hadoop/mapred-site.xml:/opt/hadoop/etc/hadoop/mapred-site.xml
    networks:
      - cluster

  postgres:
    image: postgres:17.0
    restart: on-failure
    container_name: postgres
    hostname: postgres
    environment:
      POSTGRES_DB: 'metastore_db'
      POSTGRES_USER: 'hive'
      POSTGRES_PASSWORD: 'password'
    ports:
      - '5432:5432'
    volumes:
      - hive-db:/var/lib/postgresql
    networks:
      - cluster

  metastore:
    image: apache/hive:4.0.1
    depends_on:
      - postgres
    restart: on-failure
    container_name: metastore
    hostname: metastore
    environment:
      DB_DRIVER: postgres
      SERVICE_NAME: 'metastore'
      SERVICE_OPTS: '-Xmx1G -Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver
                          -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres:5432/metastore_db
                          -Djavax.jdo.option.ConnectionUserName=hive
                          -Djavax.jdo.option.ConnectionPassword=password'
    ports:
      - '9083:9083'
    volumes:
      - warehouse:/opt/hive/data/warehouse
      - ./hive/jars/postgresql-42.7.3.jar:/opt/hive/lib/postgres.jar
    networks:
      - cluster

  hiveserver2:
    image: apache/hive:4.0.1
    depends_on:
      - namenode
      - datanode
      - metastore
    restart: on-failure
    container_name: hiveserver2
    environment:
      HIVE_SERVER2_THRIFT_PORT: 10000
      SERVICE_OPTS: '-Xmx1G -Dhive.metastore.uris=thrift://metastore:9083
                      -Dhive.security.metastore.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider'
      IS_RESUME: 'true'
      SERVICE_NAME: 'hiveserver2'
    ports:
      - '10000:10000'
      - '10002:10002'
    volumes:
      - warehouse:/opt/hive/data/warehouse
      - ./hive/jars/:/opt/hive/aux-jars/
      - ./hive/hive-site.xml:/opt/hive/conf/hive-site.xml
      - ./hadoop/hdfs-site.xml:/opt/hive/conf/hdfs-site.xml
      - ./hadoop/yarn-site.xml:/opt/hive/conf/yarn-site.xml
      - ./hadoop/mapred-site.xml:/opt/hive/conf/mapred-site.xml
      - ./hadoop/core-site.xml:/opt/hive/conf/core-site.xml
    networks:
      - cluster

  flink-iceberg-sink-job:
    container_name: flink-iceberg-sink-job
    restart: on-failure
    build:
      context: ./flink-consumer
      dockerfile: Dockerfile
    depends_on:
      namenode:
        condition: service_healthy
    ports:
      - "8081:8081"
    command: standalone-job --job-classname com.job.FootballMatchStatsTableSinkJob --job-config /opt/flink/usrlib/config-docker.yml --jars /opt/flink/usrlib/flink-consumer-1.0.jar
    volumes:
      - ../libs/:/opt/flink/usrlib/
      - ../flink-consumer/src/main/resources/config-docker.yml:/opt/flink/usrlib/config-docker.yml
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: flink-iceberg-sink-job
        parallelism.default: 2
    networks:
      - cluster

  taskmanager:
    restart: on-failure
    build:
      context: ./flink-consumer
      dockerfile: Dockerfile
    container_name: flink-taskmanager
    depends_on:
      - flink-iceberg-sink-job
    command: taskmanager
    scale: 1
    volumes:
      - ../libs/:/opt/flink/usrlib/
      - ../flink-consumer/src/main/resources/config-docker.yml:/opt/flink/usrlib/config-docker.yml
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: flink-iceberg-sink-job
        taskmanager.numberOfTaskSlots: 2
        parallelism.default: 2
    networks:
      - cluster

  redis:
    image: redis:7
    container_name: superset_cache
    restart: on-failure
    volumes:
      - redis:/data
    networks:
      - cluster

  superset-db:
    env_file: superset/superset.env
    image: postgres:17.0
    container_name: superset_db
    restart: on-failure
    volumes:
      - db_home:/var/lib/postgresql/data
      - ./superset/docker-entrypoint-initdb.d:/docker-entrypoint-initdb.d
    networks:
      - cluster

  superset-init:
    image: apache/superset:4.0.2
    container_name: superset_init
    command: [ "/app/docker/docker-init.sh" ]
    env_file: superset/superset.env
    depends_on: *superset-depends-on
    volumes: *superset-volumes
    user: "root"
    networks:
      - cluster

  superset:
    env_file: superset/superset.env
    image: *superset-image
    container_name: superset_app
    command: [ "/app/docker/docker-bootstrap.sh", "app-gunicorn" ]
    user: "root"
    restart: on-failure
    ports:
      - 8088:8088
    depends_on: *superset-depends-on
    volumes: *superset-volumes
    networks:
      - cluster

volumes:
  hadoop_namenode:
  hadoop_datanode:
  hive-db:
  warehouse:
  hue-db:
  superset_home:
    external: false
  db_home:
    external: false
  redis:
    external: false

networks:
  cluster:
    name: cluster
